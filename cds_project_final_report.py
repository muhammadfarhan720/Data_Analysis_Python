# -*- coding: utf-8 -*-
"""CDS_project_final_report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mTa3N2GlIrzPwT2Jyo6yukMLcc6nNjVo

# Problem:

   The problem topic of interest is to explore and visualize the impacts of different factors on house market pricing. Also, the interest is to find out most important factors that can effect the price of a house along with highlighting some factors which can be regarded as non-significant.

#  Importance to stakeholders :

   Currently, there is substantial endeavor in developing machine learning models for predicting the house market pricing for real estate investors and owners. This work will contribute to the effort by making the understanding easier on significance of data collection of different features before model building focussing such interests. The research will also help the data scientists working in the field to understand the feature factors more distinctly and explain to the stakeholders more intuitively.

#  Research Question 1 :

   What are the most imporant features effecting the house market pricing?

#  Research Question 2 :

   Suggest some features which does not have significant effect on house pricing?

# Research Question 3 :

  Suggest some features that partially effect on the change of price of real estate buildings?

# Dataset :
  The dataset collected for the research goal was from "Ames Housing company, Iowa" which includes geographical and structural information of about 2930 (in total) sample houses along with their construction and sale period in the form of 79 explanatory features (Exlcluding 'ID' & 'Order'). The pricing of the respective houses are provided in the 'SalePrice' coloumn of the 'train.csv' dataset but not included in the 'test.csv' dataset. The 'SalePrice' column of the 'test.csv' dataset is separately available in another file 'target.csv' which is usually utilized for measuring testing accuracy of the predicted model for house pricing market. Some of the features of the dataset are explicitly sparse and thus fail to provide useful information for those samples. Thus, the dataset needs to be cleaned and preprocessed before using it for visualization/learning.

  # Read, Clean and Preprocess Data :

  At first, we read the data from both **'train.csv'** and **'test.csv'** and store it into two respective dataframes **train** and **test**.
"""

import pandas as pd

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
# train.head()

# print(df.shape)

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)


print(train.shape)


print(test.shape)

train

test

"""As can be seen, some of features of the dataset such as 'Alley','Pool QC','Fence','Misc Feature' have no values available and thus show 'NaN' for most of the samples. Also, the feature 'PID" does not seem to be useful for house price prediction. The 'Order' feature is kept for tracking the feature and sample sequence when the train and test data will be merged later for visualization."""

columns_to_drop = ['PID','Alley','Pool QC','Fence','Misc Feature']

train = train.drop(columns=columns_to_drop)

test=  test.drop(columns=columns_to_drop)

print(train.shape)

print (test.shape)

"""The train dataframe seems to have one more extra column which is actually the target of our research 'SalePrice'. The 'SalePrice' column of test dataframe is in another 'target.csv' file which will be merged all together for visualization in later steps. We print all the column heads of both dataframes."""

print(train.columns.values)

print(test.columns.values)

"""After dropping the most sparse columns detected by having an eye glance, we focus on searching for features with missing values and percentage of such cases."""

nan_ratio = train.isna().mean()

nan_ratio_sorted = nan_ratio.sort_values(ascending=False)


print(nan_ratio_sorted)

"""It can be seen that almost half **(48.5%)** of the samples from **'Fireplace Qu'** feature are unavailable which might not be useful for estimation later at all. So the mentioned feature is dropped."""

columns_to_drop = ['Fireplace Qu']

train = train.drop(columns=columns_to_drop)

test=  test.drop(columns=columns_to_drop)

print(train.shape)

print (test.shape)

"""Now let us have a look at the rest of the features and their missing sample status :"""

nan_ratio = test.isna().mean()

nan_ratio_test_sorted = nan_ratio.sort_values(ascending=False)

print(nan_ratio_test_sorted)

"""Except for the **'Lot Frontage'** feature, the rest of the feature with missing values with very few percantage. Thus, it was decided to fill those missing values with 0 for the rest of the features except 'Lot Frontage, which was later filled with it's median value from the neighbourhood houses by grouping it with 'Neighborhood' feature. It was done because **'Lot Frontage'** which means linear feet of street connected to property should be almost equivalent to the neighborhood houses."""

for col in ('Garage Yr Blt', 'Garage Area', 'Garage Cars', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF','Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath', 'Mas Vnr Area'):
    train[col] = train[col].fillna(0)



# Fill missing values in 'Lot Frontage' with the median grouped by 'Neighborhood'
train["Lot Frontage"] = train.groupby("Neighborhood")["Lot Frontage"].transform(lambda x: x.fillna(x.median()))


# Identify numerical and categorical columns
#numerical_columns = df.select_dtypes(include=['number']).columns

categorical_columns = train.select_dtypes(exclude=['number']).columns

mode_values = train[categorical_columns].mode().iloc[0]

# Fill missing values in categorical columns with the mode
train[categorical_columns] = train[categorical_columns].fillna(mode_values)



for col in ('Garage Yr Blt', 'Garage Area', 'Garage Cars', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF','Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath', 'Mas Vnr Area'):
    test[col] = test[col].fillna(0)




# Fill missing values in 'Lot Frontage' with the median grouped by 'Neighborhood'
test["Lot Frontage"] = test.groupby("Neighborhood")["Lot Frontage"].transform(lambda x: x.fillna(x.median()))



categorical_columns = test.select_dtypes(exclude=['number']).columns

mode_values = test[categorical_columns].mode().iloc[0]

# Fill missing values in categorical columns with the mode
test[categorical_columns] = test[categorical_columns].fillna(mode_values)

"""Since almost all the missing features are compensated, let us focus on looking the quality of data available inside features. By having a glance of the dataframes, it was observed that the **'Utilities'** and **'Land Contour'** features have mostly redundant data. Let us confirm using a Barchart plot of the categorical values inside both features."""

import matplotlib.pyplot as plt

# Assuming 'train' is your DataFrame

# Calculate the percentage distribution of each category in 'Utilities'
utilities_percentage = train['Utilities'].value_counts(normalize=True) * 100

# Plot the histogram
utilities_percentage.plot(kind='bar', color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Utilities')
plt.ylabel('Percentage')
plt.title('Percentage Distribution of Utilities')

# Show the plot
plt.show()


import matplotlib.pyplot as plt

# Assuming 'train' is your DataFrame

# Calculate the percentage distribution of each category in 'Utilities'
utilities_percentage = train['Land Contour'].value_counts(normalize=True) * 100

# Plot the histogram
utilities_percentage.plot(kind='bar', color='skyblue', edgecolor='black')

# Add labels and title
plt.xlabel('Land Contour')
plt.ylabel('Percentage')
plt.title('Percentage Distribution of Land Contour')

# Show the plot
plt.show()

"""Now since more than 90% of the data for both features are redundant, the features were decided to be dropped as they have no variability inside the features and thus will not be able to have a noticeable impact on the pricing of the houses. Moreover, a mistake that was done previously by not joining the two dataframes except the **'SalePrice'** which resulting in writing codes for cleaning the frames twice. This mistake is corrected in the following segment code which formed a joined dataframe named 'result_df'. Let us also confirm that there is not missing data in the joined dataframe."""

train = train.drop(['Utilities','Land Contour'], axis=1)

test = test.drop(['Utilities','Land Contour'], axis=1)


print(train.shape[0])


print(test.shape[0])

ntrain=train.shape[0]

ntest=test.shape[0]



train_SalePrice=train['SalePrice']

train = train.drop(['SalePrice'], axis=1)

# Concatenate the two DataFrames along the rows
result_df = pd.concat([train, test], ignore_index=True)

result_df_na = (result_df.isnull().sum() / len(result_df)) * 100

#Dropped the indexes which contain no missing values
result_df_na = result_df_na.drop(result_df_na[result_df_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :result_df_na})
missing_data.head()

"""It can be seen that some portion of the **Lot Frontage** still do not have any info since there was not enough information that could be collected from the **'Neighborhood'** feature. So it was decided to fill those data with median of the column data 'Lot Frontage'. After that, the missing values were inspected again for confirmation and no missing data was found."""

# Identify rows with missing values in the 'Lot Frontage' column
missing_lot_frontage = result_df[result_df['Lot Frontage'].isnull()]

#print(missing_lot_frontage)


lot_frontage_median = result_df['Lot Frontage'].median()


# Fill missing values in the 'Lot Frontage' column with the median
result_df['Lot Frontage'].fillna(lot_frontage_median, inplace=True)


result_df_na = (result_df.isnull().sum() / len(result_df)) * 100
result_df_na = result_df_na.drop(result_df_na[result_df_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio': result_df_na})
missing_data.head()

"""There are three feature columns which include information about **basement, 1st and 2nd floor area**. A **total square feet area** was calculated by summing all those information and a new feature was formed which included this data named as **'TotalSF'**. Moreover, values of certain columns were converted from 'object' to 'str'."""

result_df['MS SubClass'] = result_df['MS SubClass'].apply(str)
result_df['Overall Cond'] = result_df['Overall Cond'].astype(str)
result_df['Yr Sold'] = result_df['Yr Sold'].astype(str)
result_df['Mo Sold'] = result_df['Mo Sold'].astype(str)

result_df['TotalSF'] = result_df['Total Bsmt SF'] + result_df['1st Flr SF'] + result_df['2nd Flr SF']
result_df

"""On this step, the 'SalePrice' target column was joined for both train and test dataframe and a final combined dataframe named as 'df' was formed from unification of 'result_df' and 'SalePrice' for all the train and test samples. The 'SalePrice' of the test dataset was read from the 'target.csv' file since it was unavailable on the respective 'test.csv'"""

# df = result_df[:ntrain].copy()  # create a copy to avoid SettingWithCopyWarning
# df['SalePrice'] = train_SalePrice

# result_df.to_csv('clean_result_df-hpp.csv', index=False)
# df.to_csv('clean_train-hpp.csv', index=False)
# df.head()

# df.shape


# print(train_SalePrice)


target_SalePrice=pd.read_csv('/content/target.csv')

test=target_SalePrice.SalePrice



final_target=pd.concat([train_SalePrice,test],axis=0)

final_target.shape

# Reset index for both DataFrames
#result_df = result_df.reset_index(drop=True)

final_target = final_target.reset_index(drop=True)

# Concatenate DataFrames along the columns (axis=1)
df = pd.concat([result_df, final_target], axis=1)

df

df.to_csv('clean_train_test-hpp.csv', index=False)

# print(result_df.columns.tolist())

# print(type(final_target))

df.head()


df.shape

"""Next, the features which have more than 50 percent  correlation with 'SalePrice' were determined to have an idea."""

corrmat = df.corr()
def get_correlated_features(corrdata, threshold):
    feature = []
    value = []
    for i , index in enumerate(corrdata.index):
        if abs(corrdata[index]) > threshold:
            feature.append(index)
            value.append(corrdata[index])
    df2 = pd.DataFrame(data = value, index=feature, columns=['corr value'] )
    return df2
corr_df = get_correlated_features(corrmat['SalePrice'], 0.5)
corr_df=corr_df.sort_values(by=['corr value'],ascending=False)

corr_df

"""It can be seen that 11 features were found to be highly correlated with house pricing. We can check those feature columns at a glance:"""

correlated_data = df[corr_df.index]
correlated_data.head()

"""Now let us plot a heatmap of these highly correlated features to have an idea of the factors that influence house pricing."""

import seaborn as sns

fig, ax = plt.subplots(figsize=(11, 11))
sns.heatmap(correlated_data.corr(), annot = True, annot_kws={'size': 12}, square=True, linecolor='w', linewidths=0.1)

"""From the heatmap, it can be observed that **overall quality of  material and finish** was the most correlated feature on house pricing. The newly synthesized feature **'TotalSF'** which refers as total square feet area also came to second most correlated feature to house pricing.. The **general living area, garage area** were also found to be very important correlated feature behind price change of house pricing.

Since the processing of data has been almost completed, let us check the distribution of the target **'SalePrice'** before we start analysis.
"""

from scipy import stats
from scipy.stats import norm, skew
import matplotlib.pyplot as plt
import numpy as np


sns.distplot(df['SalePrice'] , fit=norm);
(mu, sigma) = norm.fit(df['SalePrice'])
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')
fig = plt.figure()
res = stats.probplot(df['SalePrice'], plot=plt)
plt.show()

"""It seems that the **"SalePrice"** column is highly right skewed and the probability plot is hardly linear which might have a negative impact on data analysis with visualization later. Let us make the distribution normal with logarithmic transformation which also results in making the probability plot linear and good for visualization analysis."""

df["SalePrice"] = np.log1p(df["SalePrice"])

sns.distplot(df['SalePrice'] , fit=norm);
(mu, sigma) = norm.fit(df['SalePrice'])
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')
fig = plt.figure()
res = stats.probplot(df['SalePrice'], plot=plt)
plt.show()

"""# Analysis (Bivariate) :

We start the analysis with visualizing relationships between **'SalePrice'** and another feature under experiment. Let us first choose the most correlated variable feauter with the target data named as 'Overall Qual'. A scatterplot is generated with a best fitted regression line model betweel  **'SalePrice'** vs **'Overall Qual'**.


"""

import seaborn as sns
sns.lmplot(y='SalePrice',x='Overall Qual',data=df)

"""The scatter points represent individual data samples and the regression line represent an average estimation over the relationship between twa variables. As can be seen from both, there is clearly linear relatiohship between overall quality ratings with pricing of house since with increase of quality the price also seems to increase in a linear manner.

Let us investigate the relationship between pricing and another highly correlated feature **'Garage Area'**. A joint plot is used to demonstrate both univariate and bivariate plot of variables. The bivariate plot uses a scatterplot and the univariate histograms show distribution of individual features.
"""

import seaborn as sns

sns.jointplot(y='SalePrice',x='Garage Area',data=df)

plt.show

"""From the above plot, it can be observed that price of houses has linear positive correlation with garage area size. The scatterplot is also pretty congested in a cluster which represents high correlation while there are some outliers around the plot.The Garage area distribution data seems to be skewed towards increased garage area. The 'SalePrice' distribution seems to be skewed towards lower price range representing there is not many samples in the dataset in relatively lower price range.

Now, let us witness the relation between a variable **'Yr Sold'** which represents the time period when the properties were sold and **'SalePrice'**. A scatterplot with jointplot is generated to understand the relationship
"""

import seaborn as sns

df_sorted = df.sort_values(by='Yr Sold',ascending=True)

sns.jointplot(y='SalePrice', x='Yr Sold', data=df_sorted)

"""As can be seen from the plot, there is no special relation between the price of the house and the time in which they were sold. It can be noticed from the distribution of 'Yr Sold' that the number of properties sold in 2010 decreased drastically after the end of recession which lasted from 2007 to 2009. It was also seen that the outliers in 2010 for pricing of houses were comparitively lower than the other years which signifies that people stopped selling the houses in discounted prices as the recession was over. But no significant relation with house pricing increase was seen over the years plotted from the dataset.

Let us examine now the contruction or remodelling period **'Year Remod/Add'** relationship with pricing. A linear regression model line with scatterplot is plotted below :
"""

sns.lmplot(x='SalePrice', y='Year Remod/Add', data=df)

"""It was noticed that there is almost a linear relationship with construction/remodelling year with pricing. I did not actually expect to see a relation here. Seems that period of the sale year has less impact on price of houses than period of construction/remodelling. The more 'recent' the construction/remodelling is, the higher the price.

After this, we plot the relationship between pricing and number of bathroom in houses named as **'Full Bath'**. Let us examine the relationship by generating a strip plot :
"""

import matplotlib.pyplot as plt

# Create a stripplot or swarmplot
plt.figure(figsize=(16, 11))  # Adjust the figure size if needed

# Stripplot
sns.stripplot(x='Full Bath', y='SalePrice', data=df, jitter=True, alpha=0.7)

plt.title('SalePrice Distribution by Neighborhood (Stripplot)')
plt.show()

"""The above plot suggests that the pricing generally increases with the inceased number of bathroom in apartments largely upto count of 3. After that, more number of bathroom increase does not impact the pricing significantly.

Next, I wanted to investigate the **neighborhood locations** of the properties and their impact on pricing. For this reason, another strip plot is formed :

"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a stripplot or swarmplot
plt.figure(figsize=(16, 11))  # Adjust the figure size if needed

# Stripplot
sns.stripplot(x='Neighborhood', y='SalePrice', data=df, jitter=True, alpha=0.7)
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.title('SalePrice Distribution by Neighborhood (Stripplot)')
plt.show()

"""From the plot it can be estimated that, neighborhood does have a significant impact on the pricing of the homes. Although there are outliers in most of the neighborhood, the in general most of the properties remain at similar price range for individual neighborhoods. As the samples are almost clustered very closely, it can be measured that the neighborhood within high price range, has most of the properties in expensive region whereas neighborhoods with relatively medium or lower price ranges has majority samples of it in medium or lower price region respectively.

I was also curious to analyze the relation between **garage year** period and pricing. So I plotted their relationship and found that it was highly negatively correlated and hence no trend could be explained.
"""

sns.relplot(x='SalePrice', y='Garage Yr Blt', data=df)

"""# Multivariate plots

Let us now examine relationship among more than two variables. A bubble plot among **total square feet area, price and living area size** was generated where all of these features have proven to be highly correlated with each other.
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.scatter(x=df['SalePrice'], y=df['TotalSF'],s=df['Gr Liv Area']/50,alpha=.5)




plt.title('Scatter Plot of SalePrice vs TotalSF with Gr Liv Area as Size')
plt.xlabel('SalePrice')
plt.ylabel('TotalSF')
plt.show()

"""It can be seen that there is clearly a  linearly relation between total square feet area and pricing of the house as the bubbles are clustered in a congested location signifying the high correlation and their increase of position in vertical axis with the increase of price. Also, it can also be observed that lower living space results in lower price and higher living space size causes higher prices since the bubbles sizes represent the living area size. It can also be noticed that some of the properties with highest living area, still could not make the pricing highest, which can be regarded as an outlier.

Now let us consider other two highly correalted features as **overall quality rating and garage car number** and plot their relationship with a linear regression model fit :
"""

sns.lmplot(x='Overall Qual', y='SalePrice', data=df, hue= 'Garage Cars')

"""The above plot represent the linear regression model fit between 'SalePrice' and 'Overall Qual' on the basis of data grouped by 'Garage Cars' which is represented by different colors. As the plot seems to be almost vague to understand, it was broken into several individual plots below :"""

sns.lmplot(x='Overall Qual', y='SalePrice', data=df, col= 'Garage Cars')

"""It can be noticed that the scatterpoints are moving towards right with the increase of garage car number, which represents the increase of quality rating of house with it's garage car capacity. The exception was for garage cars=4 where the quality was not higher than the previous case. The uncertainty of the linear model can also be noted as there are not sufficient samples to make the regression line fit. For garage cars=5 there is only one sample making it unusable to estimate or learn something from that plot.

#Report :

In this project,  impacts of various factors were explored over house pricing. After investigation, the highest correlated features with 'SalePrice' were found to be overall quality rating, total square feet area, living area size and garage car capacity. It was also found that some of the features like year of construction/remodelling, bathroom-count and neighborhood have partial impact on the pricing of houses. Some of features which were seemed to have no signifant impact or highly negative correlated with pricing were saling period of properties and year when garage was built.

Thus, the investigation ends here. More feature inspection can be done later extensively with more data inclusion and feature relationship research if necessary.


"""